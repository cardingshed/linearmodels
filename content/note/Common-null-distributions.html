---
categories:
- Unit-1
slug: common-null-distributions
tags:
- normal
- chi-squared
- t-distribution
- F-distribution
- Central-limit-theorem
- Law-of-large-numbers
title: Intuitive background for parameter distributions
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>How to understand which distribution is used when and why it’s in that test…</p>
<p>Often, parameters in linear modelling can be assumed to follow a known distribution if a <em>null-hypothesis</em> is true, which is the justification for e.g. Wald-tests (normal), chi-squared tests, <span class="math inline">\(t\)</span>-tests, and F-tests.</p>
<p>The <a href="#normal-link">normal</a>, <a href="#chi-link">chi-squared</a>, <a href="#t-link">t</a> and <a href="#F-link">F</a> distributions are the most commonly met as <em>null-distributions</em> in these tests - this is a brief guide to help build an intuitive understanding of why they arise.</p>
<div id="normal-link" class="section level1">
<h1>Normal Distribution</h1>
<p>Two famous results in statistics help explain the importance of the mean, (=<em>expected value</em>), and the normal distribution:</p>
<div id="the-law-of-large-numbers" class="section level2">
<h2>The law of large numbers</h2>
<p>Taking multiple readings and averaging them produces a more accurate estimate. A famous practical example is when Michelson was measuring the <a href="https://en.wikipedia.org/wiki/Albert_A._Michelson#Speed_of_light">speed of light</a>. To see why this is so consider the variance:</p>
<p>With one experimental set of data <span class="math inline">\({X_1}\)</span> to estimate a value <span class="math inline">\(X\)</span>, we might have <span class="math inline">\(\mathbb{E}(X)=\mu\)</span> and <span class="math inline">\(\mathbb{V}(X)=\sigma^2\)</span>.
However, we can more accurately zero-in on <span class="math inline">\(\mu\)</span> by considering the expectation of multiple averages - if we have <span class="math inline">\(n\)</span> independent sets of data <span class="math inline">\({X_i}\)</span> then <span class="math display">\[\mathbb{E}(\bar{X_i})=\mathbb{E}(\frac{1}{n}\sum_{i=1}^n X_i)=\frac{1}{n}\sum_{i=1}^n \mathbb{E}(X_i)=\frac{1}{n}n\mu=\mu\]</span> (still!), but crucially…
<span class="math display">\[\mathbb{V}(\bar{X_i})=\mathbb{V}(\frac{1}{n}\sum_{i=1}^n X_i)=\frac{1}{n^2}\sum_{i=1}^n \mathbb{V}(X_i)=\frac{1}{n^2}n\sigma^2=\frac{\sigma^2}{n}\]</span> so <span class="math inline">\(\bar{X_i}\)</span> is probably <em>closer</em> to <span class="math inline">\(\mu\)</span> than <span class="math inline">\(\bar{X_1}\)</span> is - the standard deviation reduces by a factor of <span class="math inline">\(\sqrt{n}\)</span> as <span class="math inline">\(n\)</span> becomes a larger number. Given sufficiently large <span class="math inline">\(n\)</span>, we can get arbitrarily close to <span class="math inline">\(\mu\)</span>!</p>
</div>
<div id="the-central-limit-theorem" class="section level2">
<h2>The Central Limit Theorem</h2>
<p>This is the super-charged version of the law of large numbers, capturing more precisely how <span class="math inline">\(\bar{X_n}\)</span> approaches <span class="math inline">\(\mu\)</span> as <span class="math inline">\(n\)</span> increases.</p>
<ul>
<li>the sample mean as a variable, and how it relates to the distribution mean</li>
<li>it’s an asymptotic result, so relies on “large” sample sizes</li>
</ul>
<p>The central limit theorem is amazingly general: the distribution of the sample mean converges to a normal distribution even if the underlying distribution of each datapoint is discrete or bimodal (it does converge more slowly then though…) so in practice a normal distribution can often be used to model the behaviour of an expected value.</p>
<p>As <span class="math inline">\(n \to \infty\)</span>:
<span class="math display">\[\bar{X_n} \sim \mathcal{N}(\mu, \frac{\sigma^2}{n})\]</span>
Equivalently, <span class="math inline">\(\frac{\bar{X_n}-\mu}{\sqrt{\sigma^2/n}}\sim \mathcal{N}(0,1)\)</span>. Although the population variance <span class="math inline">\(\sigma^2\)</span> is often unknown, a clever trick enables us to use the sample variance instead - with the <a href="#t-link">t-distribution below</a></p>
</div>
</div>
<div id="chi-link" class="section level1">
<h1>chi-squared Distribution</h1>
<p>The origin of the <span class="math inline">\(\chi^2\)</span> distribution is as the sum of independent squared standard normal variables, i.e. if</p>
<p><span class="math inline">\(Y = Z_1^2 + Z_2^2 + \dots Z_n^2\)</span>, where <span class="math inline">\(Z_i \sim \mathcal{N}(0,1)\)</span>, then <span class="math inline">\(Y \sim \chi^2(n)\)</span>, a <em>chi-squared distribution with <span class="math inline">\(n\)</span> degrees of freedom</em>. Note the relationship with the CLT: as more terms are added the chi-squared distribution (slowly) becomes more like the normal.</p>
<p><img src="/note/Common-null-distributions_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>If <span class="math inline">\(X \sim \mathcal{N}(\mu,\sigma^2)\)</span>, we can intuitively see how a chi-squared distribution is related to sample variance:
<span class="math inline">\(S^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \mu)^2\)</span> which implies <span class="math inline">\(\frac{S^2}{\sigma^2}=\frac{1}{n-1}\sum_{i=1}^{n}(\frac{X_i - \mu}{\sigma})^2=\frac{1}{n-1}\sum_{i=1}^{n}Z_i^2\)</span> where <span class="math inline">\(Z_i \sim \mathcal{N}(0,1)\)</span>. Note that in practice <span class="math inline">\(\mu\)</span> is not known but estimated by the sample mean <span class="math inline">\(\bar{X}\)</span> - this is why (hand-wavy stats!) there is one degree of freedom less (very typical that for each parameter we estimate we lose a degree of freedom) and we have the result <span class="math display">\[\frac{(n-1)S^2}{\sigma^2} \sim \chi^2(n-1)\]</span>
For a more formal proof <a href="https://online.stat.psu.edu/stat414/lesson/26/26.3">see here</a></p>
</div>
<div id="t-link" class="section level1">
<h1>t Distribution</h1>
<p>The <span class="math inline">\(t\)</span> distribution is closely related to the normal distribution - it’s flatter, with more probability mass in the tails. As the degrees of freedom increase, it moves closer and closer to the standard normal, until <span class="math inline">\(t(\infty)=\mathcal{N}(0,1)\)</span>.</p>
<p><img src="/note/Common-null-distributions_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Having <em>degrees of freedom</em> is a clue that a <span class="math inline">\(\chi^2\)</span> distribution might be involved somewhere, and indeed if <span class="math inline">\(Z \sim \mathcal{N}(0,1)\)</span> and <span class="math inline">\(V \sim \chi^2(\nu)\)</span> then
<span class="math display">\[ T = \frac{Z}{\sqrt{(V/\nu)}}\]</span> has a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(\nu\)</span> degrees of freedom.</p>
<p>Useful example: <span class="math inline">\(Z = (\bar{X_n}-\mu)(\sqrt\frac{n}{\sigma^2})\)</span> is standard normal by the central limit theorem, and <span class="math inline">\(V = (n-1)\frac{S_n^2}{\sigma^2}\)</span> is chi-squared with <span class="math inline">\((n-1)\)</span> degrees of freedom as above, so:
<span class="math display">\[ T = \frac{Z}{\sqrt{V/(n-1)}} = (\bar{X_n}-\mu)(\sqrt\frac{n}{S_n^2}) \]</span> has a <span class="math inline">\(t\)</span>-distribution with (n-1) degrees of freedom, which is very useful because the unknown <span class="math inline">\(\sigma^2\)</span> terms have cancelled out and <span class="math inline">\(S_n^2\)</span> is the sample variance which we can calculate from the data.</p>
</div>
<div id="F-link" class="section level1">
<h1>F Distribution</h1>
<p>The F distribution comes from the ratio of two chi-squared random variables. Recall from above that chi-squared is typically associated with estimating a variance parameter (like <span class="math inline">\(\sigma^2\)</span>), and indeed we meet the F distribution when we wish to compare two variances. One classic example is checking the assumption of equal variance in a two sample <span class="math inline">\(t\)</span>-test, or its more general version “ANOVA” - which is unsurprising if we know that ANOVA stands for <em>analysis of variance</em>.</p>
<p>If <span class="math inline">\(Y_1 \sim \chi^2(\nu_1)\)</span> and <span class="math inline">\(Y_2 \sim \chi^2(\nu_2)\)</span> then we have:
<span class="math display">\[ F = \frac{Y_1/\nu_1}{Y_2/\nu_2} \sim F(\nu_1,\nu_2)\]</span> An F distribution with <span class="math inline">\(\nu_1\)</span> and <span class="math inline">\(\nu_2\)</span> degrees of freedom.
<span style="color: red;">Caution</span>: the order that <span class="math inline">\(Y_1\)</span>, <span class="math inline">\(Y_2\)</span> are chosen does matter, conventionally the choice is made so that the ratio is greater than one, and the <span class="math inline">\(p\)</span>-value quoted twice the one-sided <span class="math inline">\(P(F&gt;Y_1/Y_2)\)</span>.</p>
</div>
