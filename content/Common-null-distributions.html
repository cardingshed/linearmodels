---
anchor: common-null-distributions
title: Normal, chi-squared, t and F distributions
weight: 50
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>How to understand which distribution is used when and why it’s in that test…</p>
<p>Often, parameters in linear modelling can be assumed to follow a known distribution if a <em>null-hypothesis</em> is true, which is the justification for e.g. Wald-tests (normal), chi-squared tests, t-tests, and F-tests.</p>
<p>The normal, chi-squared, t and F distributions are the most commonly used distributions in tests in linear modelling - this is a brief guide to help build an intuitive understanding of why they arise.</p>
<div id="normal-distribution" class="section level1">
<h1>Normal Distribution</h1>
<p>Two famous results in statistics help explain the importance of the mean, (<em>expected value</em>), and the normal distribution:</p>
<p><strong>The law of large numbers</strong></p>
<p>It’s been known for many centuries that taking multiple readings and averaging them produces a better estimate, for example when Michelson was measuring the <a href="https://en.wikipedia.org/wiki/Albert_A._Michelson#Speed_of_light">speed of light</a>. To see why this is so consider the variance:</p>
<p><strong>The Central Limit Theorem</strong></p>
</div>
<div id="chi-squared-distribution" class="section level1">
<h1>chi-squared Distribution</h1>
</div>
<div id="t-distribution" class="section level1">
<h1>t Distribution</h1>
</div>
<div id="f-distribution" class="section level1">
<h1>F Distribution</h1>
<p>The F distribution comes from the ratio of two chi-squared random variables. Recall from above that chi-squared is typically associated with estimating a variance parameter (like sigma^2), and indeed we meet the F distribution when we wish to compare two variances. The classic example is in “ANOVA” analysis - which is unsurprising if we know that ANOVA stands for <em>analysis of variance</em>.</p>
</div>
